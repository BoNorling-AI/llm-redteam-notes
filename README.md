# LLM Red Teaming Practice Log

This repository documents my learning process in AI red teaming.

Goals:
- Understand adversarial prompting
- Learn evaluation methodology
- Build reproducible test cases
- Study failure modes in language models

---

## What is Red Teaming?

Red teaming is the process of intentionally stress-testing a system to find weaknesses.

In AI systems, this includes:
- Prompt injection attempts
- Roleplay jailbreak attempts
- Multi-turn degradation
- Instruction hierarchy attacks
- Obfuscation strategies

---

## Learning Plan

Phase 1:
- Study attack categories
- Document example attack templates
- Log model responses

Phase 2:
- Build structured test cases
- Track refusal vs compliance
- Analyze failure patterns

Phase 3:
- Automate scoring
- Compare across models
- Evaluate mitigation strategies
